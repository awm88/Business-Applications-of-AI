# -*- coding: utf-8 -*-
"""MMAI5040 – W21 –Group3 – Assignment1

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1PhypeavQlRApjajc8QtkGmeHbUDaDy1-

Group 3:

Awmnah Asad

Qiuying Duan

Amir Haji Seyed Hassani

Carlo Muser

Xiaotong Xu

Jiachen Zhao

# **Q1**

## a)
"""

# load required libraries

import pandas as pd
import matplotlib.pylab as plt
import seaborn as sns
from pandas.plotting import scatter_matrix, parallel_coordinates

import warnings
warnings.filterwarnings("ignore",category=DeprecationWarning)

from google.colab import files
uploaded = files.upload()

# load data
Cereals_df = pd.read_csv('Cereals.csv')

# find dimensions of the data frame
Cereals_df.shape

# show the 1st 10 rows
Cereals_df.head(10)

# a. Which variables are quantitative/numerical? Which are ordinal? Which are nominal?
# quantitative/numerical：'calories','protein','fat','sodium',‘fiber’,'carbo','sugars','potass','vitamins','weight','cups','rating'
# ordinal：'shelf'
# nominal：'mfr','type'

"""## b)"""

# Quantitative variable statistcs:
drop_co=['mfr','type','name','shelf']
Cereals_quant= Cereals_df.drop(drop_co,axis=1)

Cereals_quant.head()

Cereals_quant.describe()

Cereals_quant.median()

# Mean, median, min, max,standard deviation show in the above graph

"""## c)"""

figure ,axes =plt.subplots(13,figsize=(5,25),dpi=100)

for columns, ax in zip(Cereals_quant, axes):
    Cereals_quant[columns].plot.hist(ax=ax, title=columns,bins=5)

plt.tight_layout()    
plt.show()

Cereals_quant.std().sort_values(ascending=False)

#Plot a histogram for each of the quantitative variables. Based on the histograms and
#summary statistics, answer the following questions:
#i. Which variables have the largest variability?
#ii. Which variables seem skewed?
#iii. Are there any values that seem extreme?

# i. sodium and potass
# ii. protein, fat, sodium,fiber,potass,rating, cups
# iii. fiber, potass, rating

"""## d)"""

CH_cal=Cereals_df[['type','calories']]
CH_cal.tail()

ax = CH_cal.boxplot(column='calories', by='type')
ax.set_ylabel('calories')
plt.suptitle('') 
plt.title('')

# What does this plot show us?

# The median of hot cereals is 100 calories
# The median of cold cereals is approx 110 calories
# the interquartile range for Cold cereals calories is between around 100 to 110 calories
# Most of the Cold cereals calories data is fall into the range of 90 to 120 calories
# Most of data are cold cerial. 
# Cold has long tails(a few extrem valus) and right skewed 
# there are at least 7 outliers in Cold cereals
# There is only a few Hot data and the distribution is concentrated

"""## e)"""

CR_SH=Cereals_df[['shelf','rating']]
CR_SH.head()

ax = CR_SH.boxplot(column='rating', by='shelf')
ax.set_ylabel('rating')
plt.suptitle('') 
plt.title('')

# we can get rid of shelf 1 or 3, or combine shelf 1 and 3 together.
# the rating distribution of shelf 1 and 3 is similar, for prediction purposes, using
# one of the shelf can be sufficient.
# so keep 2 and 1 or keep 2 and 3 is good. or combine 1 and 3 and keep with 2

"""## f)"""

corr = Cereals_quant.corr()
corr

fig, ax = plt.subplots(figsize=(15,15))        
sns.heatmap(corr, xticklabels=corr.columns, yticklabels=corr.columns, annot=True, linewidths=0.5, ax=ax)

sns.set_theme(style="ticks")
sns.pairplot(Cereals_quant)

"""# **Q2**

## a)
"""

#confusion matrix

from mlxtend.plotting import plot_confusion_matrix
import matplotlib.pyplot as plt
import numpy as np

cm = np.array([[920, 32],
               [58, 30]])

figure, ax = plot_confusion_matrix(conf_mat = cm)
plt.show()

#error rate
ERR = ((58+32)/(58+32+30+920))
print ("Error rate: {}%".format(ERR*100))

"""## b)

i. Raising the cutoff would lead to fewer records classified as fraudulent, decreasing the error rate. Lowering the cutoff would have the opposite effect, reducing the error rate.

ii. Raising the cutoff would lead to more records classified as nonfraudulent, decreasing the error rate. Lowering the cutoff would have the opposite effect, reducing the error rate.

## c)

i. A decile lift chart evaluates the performance of a classification model. The bars show the factor by which our model outperforms a random assignment of postives and negatives. The two left most columns represent the first two deciles, i.e. equal sized bars, together representing 20% of the scored data. As the data is sorted left-to-right by predicted probability of positive outcome, the two left bars represent data predicted by the model to most likely to be nonfraudulent.

ii. Given the above, a model that excels in predicting outcomes should see a "staircase" effect in its decile lift chart, that is a large first decile column, followed by decresing column sizes for the remaining deciles. While looking at the data might give an overall picture of the average rate of fraud in records, a decile lift chart breaks down which subgroups of records are most fraudulent

iii. Classifying all outcomes as nonfraudulent drops the error rate to ~8.46%

iv. As the above data shows, the usefulness of an error-rate calculation becomes less useful in a situation with very few 'positive' records. By classifying all points 'negative', or non-fraudulent, the error rate decreased (a seemingly desired outcome) but our model actually became *less* accurate at predicting fraud. The decile lift chart on the other hand immediately points out the model's strong performance by its staircase shape - any 'outlier' bars would indicate a potentially hidden deficiency in model accuracy.

#**Q3**
"""

from google.colab import files
uploaded = files.upload()

"""## a)"""

# Commented out IPython magic to ensure Python compatibility.
!pip install dmba
# %matplotlib inline
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import math
from pathlib import Path
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import accuracy_score, roc_curve, auc, mean_squared_error,confusion_matrix
import seaborn as sns
from dmba import regressionSummary, exhaustive_search,classificationSummary, liftChart, gainsChart
from dmba import stepwise_selection
from dmba import adjusted_r2_score, AIC_score, BIC_score

Airfare=pd.read_csv("Airfares.csv")
Airfare.shape

Airfare.info()

Airfare.describe()

sns.heatmap(Airfare.corr(),annot=True,cmap='RdYlGn',linewidths=0.2) 
fig=plt.gcf()
fig.set_size_inches(10,8)
plt.show()

"""From the plot, it can be seen that  <br>
Distance and Coupon are more positive correlated with our preditor variable, FARE. 
"""

# Scatterplots between FARE and those predictors
for i in ['COUPON','NEW','HI',"S_INCOME",'E_INCOME','S_POP','E_POP','DISTANCE','PAX']:
    Airfare.plot.scatter(x = i, y = 'FARE')
#

"""From the correlation table and scatterplots, distance is the best single predictor

## b)
"""

# Explore the categorical predictors by computing the 
# percentage of flights in each category
category = ['VACATION','SW','SLOT','GATE']
for i in category:
    temp = Airfare.groupby([i])
    print(Airfare[i].value_counts()/ len(Airfare))

# a pivot table with the average fare in each category.
category = ['VACATION','SW','SLOT','GATE']
for i in category:
    print(pd.pivot_table(Airfare, index= i, values= "FARE"))

"""Based on the average FARE as calculated by categorical variables is the highest for constrained GATE and the lowest for SW_yes. From this we can guess that SouthWest serviced flights will be the cheapest and airports that are known for controlling their Gate will be the most expensive.

## c)

### i)
"""

drop_cols=['S_CODE','S_CITY','E_CODE','E_CITY']
Airfare.drop(columns=drop_cols, inplace=True)

Airfare.GATE = Airfare.GATE.astype('category')
Airfare.SLOT = Airfare.SLOT.astype('category')
Airfare.SW = Airfare.SW.astype('category')
Airfare.VACATION = Airfare.VACATION.astype('category')

Airfare_full = pd.get_dummies(Airfare, prefix_sep='_')
trainData, validData = train_test_split(Airfare_full, test_size=0.30, random_state=1)
print('Training   :', trainData.shape)
print('Validation :', validData.shape)

train_X = trainData.drop(columns=['FARE'])
train_y = trainData['FARE']

valid_X = validData.drop(columns=['FARE'])
valid_y = validData['FARE']

"""### ii)"""

#ii
def train_model(variables):
  if len(variables) == 0:
    return None
  model = LinearRegression() 
  model.fit(train_X[variables], train_y) 
  return model  

def score_model(model, variables):
  if len(variables) == 0:
      return AIC_score(train_y, [train_y.mean()] * len(train_y), model, df=1)
  return AIC_score(train_y, model.predict(train_X[variables]), model)

best_model, best_variables = stepwise_selection(train_X.columns, train_model, score_model,
verbose=True)
print(best_variables)

"""According to the results, **'DISTANCE', 'SW_No', 'VACATION_No', 'HI', 'SLOT_Controlled', 'GATE_Constrained', 'E_INCOME', 'PAX', 'S_POP', 'E_POP', 'S_INCOME', 'NEW'** are best predictors."""

stepwise_variable = ['DISTANCE', 'SW_No', 'VACATION_No', 'HI', 'SLOT_Controlled', 'GATE_Constrained', 'E_INCOME', 'PAX', 'S_POP', 'E_POP', 'S_INCOME', 'NEW']
stepwise_model = LinearRegression() 
stepwise_model.fit(train_X[stepwise_variable], train_y) 
stepwise_RMSE = mean_squared_error(valid_y,stepwise_model.predict(valid_X[stepwise_variable]),squared=False)

stepwise_RMSE

model = LinearRegression() 
model.fit(train_X[best_variables], train_y)
regressionSummary(valid_y, model.predict(valid_X[best_variables]))

"""### iii)"""

#Question C
#iii
def train_model(variables):
    model = LinearRegression()
    model.fit(train_X[variables], train_y)
    return model

def score_model(model, variables):
    pred_y = model.predict(train_X[variables])
    # we negate as score is optimized to be as low as possible
    return -adjusted_r2_score(train_y, pred_y, model)

allVariables = train_X.columns
results = exhaustive_search(allVariables, train_model, score_model)
data = []
for result in results:
    model = result['model']
    variables = result['variables']
    AIC = AIC_score(train_y, model.predict(train_X[variables]), model)
    d = {'n': result['n'], 'r2adj': -result['score'], 'AIC': AIC}
    d.update({var: var in result['variables'] for var in allVariables})
    data.append(d)
pd.set_option('display.width', 100)
print(pd.DataFrame(data, columns=('n', 'r2adj', 'AIC') + tuple(sorted(allVariables))))
pd.reset_option('display.width')

r=pd.DataFrame(data, columns=('n', 'r2adj', 'AIC') + tuple(sorted(allVariables)))

exhausted=list((r.iloc[10]).loc[r.iloc[10] == True].index)

"""**'DISTANCE', 'E_INCOME', 'E_POP', 'GATE_Constrained','HI','PAX','SLOT_Controlled','SW_No','S_INCOME','S_POP','VACATION_No**' Are selected features based on exhustive search."""

exhustive_variable = ['DISTANCE', 'E_INCOME', 'E_POP', 'GATE_Constrained','HI','PAX','SLOT_Controlled','SW_No','S_INCOME','S_POP','VACATION_No']
exhustive_model = LinearRegression() 
exhustive_model.fit(train_X[exhustive_variable], train_y) 
exhustive_RMSE = mean_squared_error(valid_y,exhustive_model.predict(valid_X[exhustive_variable]),squared=False)
exhustive_RMSE

import statsmodels.api as sm
sm_X = train_X[exhustive_variable]
sm_valid_X = valid_X[exhustive_variable]
sm_Y = train_y
sm_model = sm.OLS(sm_Y,sm_X)

"""Mean Squared Error is 35.54 for exhustive search whereas it is 35.54"""

sm_results = sm_model.fit()

sm_results.summary()

model = LinearRegression() 
model.fit(train_X[exhustive_variable], train_y)
regressionSummary(valid_y, model.predict(valid_X[exhustive_variable]))

"""### iv)"""

#iv
pred_s_v = pd.Series(stepwise_model.predict(valid_X[stepwise_variable]))
pred_s_v = pred_s_v.sort_values(ascending=False)

pred_e_v = pd.Series(exhustive_model.predict(valid_X[exhustive_variable]))
pred_e_v = pred_e_v.sort_values(ascending=False)

fig, axes = plt.subplots(nrows=1, ncols=2)
ax = liftChart(pred_s_v, ax=axes[0], labelBars=False)
ax.set_ylabel('Lift')
ax.set_title('Lift Chart (Stepwise Regression)')


ax = liftChart(pred_e_v, ax=axes[1], labelBars=False)
ax.set_ylabel('Lift')
ax.set_title('Lift Chart (Exhustive Search)')

plt.tight_layout()
plt.show()

"""### v)"""

#v
new_X = pd.DataFrame([{'DISTANCE':1976, 'E_INCOME': 27664, 'E_POP': 3195503,
                        'GATE_Constrained': 0,'HI':4442.141,'PAX':12782,
                        'SLOT_Free': 0,'SW_No': 1,'S_INCOME':28760,
                        'S_POP':4557004,'VACATION_No':1}])
exhustive_model.predict(new_X)

"""The predicted Fare is approximately 250.6

### vi)
"""

#vi
new_X = pd.DataFrame([{'DISTANCE':1976, 'E_INCOME': 27664, 'E_POP': 3195503,
                        'GATE_Constrained': 0,'HI':4442.141,'PAX':12782,
                        'SLOT_Free': 0,'SW_No': 0,'S_INCOME':28760,
                        'S_POP':4557004,'VACATION_No':1}])

exhustive_model.predict(new_X)

"""The Predicted Fare with SW_yes is 209.95

### vii)

Since SLOT, PAX, and GATE are calculated based on the route, those would not be available prior to starting a new route.

### viii)
"""

#viii
not_real_variable = ['PAX','GATE','SLOT']
real_Airfare = Airfare.drop(columns=not_real_variable)

Airfare_real_full = pd.get_dummies(real_Airfare, prefix_sep='_')
real_trainData, real_validData = train_test_split(Airfare_real_full, test_size=0.30, random_state=1)
print('Training   :', trainData.shape)
print('Validation :', validData.shape)

real_train_X = real_trainData.drop(columns=['FARE'])
real_train_y = real_trainData['FARE']

real_valid_X = real_validData.drop(columns=['FARE'])
real_valid_y = real_validData['FARE']

real_results = exhaustive_search(real_train_X.columns, train_model, score_model)

data = []
for result in real_results:
    model = result['model']
    variables = result['variables']
    AIC = AIC_score(train_y, model.predict(train_X[variables]), model)
    d = {'n': result['n'], 'r2adj': -result['score'], 'AIC': AIC}
    d.update({var: var in result['variables'] for var in allVariables})
    data.append(d)
pd.DataFrame(data, columns=('n', 'r2adj', 'AIC') + tuple(sorted(allVariables)))

real_best_model = None
real_variables = None
for result in real_results:
  if result['n'] == 9:
    real_best_model = result['model']
    real_variables = list(result['variables'])
    break

real_variables

"""'HI', 'E_INCOME', 'S_POP', 'E_POP', 'DISTANCE', 'VACATION_No', 'SW_No' are selected features from exhusted search.

### ix)
"""

# COUPON = 1.202, NEW = 3, VACATION = No, SW = No, HI = 4442.141, S_INCOME = 28,760, E_INCOME = 27,664, S_ POP = 4,557,004, E_POP = 3,195,503, SLOT = Free, GATE = Free, PAX = 12782, DISTANCE = 1976 miles.
real_new_X = pd.DataFrame([{'COUPON':1.202,'HI':4442.141, 'E_INCOME':27664, 'S_INCOME':28760 , 'S_POP':4557004, 'E_POP':3195503, 'DISTANCE':1976, 'VACATION_No': 1, 'SW_No':1}])

real_best_model.predict(real_new_X)

"""The predicted fare is 257.18

### x)
"""

real_exhustive_RMSE = mean_squared_error(real_valid_y,real_best_model.predict(real_valid_X[real_variables]),squared=False)
real_exhustive_RMSE

regressionSummary(real_valid_y, real_best_model.predict(real_valid_X[real_variables]))

"""RMSE for this model is 37.55 while it is 35.5 in model (iii). which shows that the current model has higher rate of error."""

sm_real_X = train_X[real_variables]
sm_real_valid_X = valid_X[real_variables]
sm_real_Y = train_y
sm_real_model = sm.OLS(sm_real_Y,sm_real_X)
sm_real_results = sm_real_model.fit()

sm_real_results.summary()

regressionSummary(real_valid_y, sm_real_results.predict(real_valid_X[real_variables]))